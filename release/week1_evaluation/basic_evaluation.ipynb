{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a07b7fe35d73787be4dc24f405a3dff4",
     "grade": false,
     "grade_id": "cell-de7f8892240622ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Introduction\n",
    "The goal of this assignment is to create a basic program that provides an overview of basic evaluation metrics (in particular precision, recall, f-score) from documents provided in the conll format. Make sure that your code can handle situation when there are no true positives for a specific class.\n",
    "\n",
    "This notebook provides a suggested structure (through functions) and modules. You may want to make use of additional functions (depending on how you structure your code). \n",
    "\n",
    "There is also an advanced version which you can follow if you prefer to structure your code differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1d033fa1402d9d88b8435ed0627f8e5",
     "grade": false,
     "grade_id": "cell-2919e8a7c347fffa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#see tips & tricks on using defaultdict (remove when you do not use it)\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_annotations(inputfile, annotationcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "    :type inputfile: string\n",
    "    :type annotationcolumn: string\n",
    "    :type delimiter: string\n",
    "    :returns: the annotations in a structured format of your choice\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29489e5d8a240c715f3f3fee7b8dfc91",
     "grade": false,
     "grade_id": "cell-f96d30b6f5cb01a5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def obtain_counts(goldannotations, machineannotations):\n",
    "    '''\n",
    "    This function compares the gold annotations to machine output\n",
    "    \n",
    "    :param goldannotations: the gold annotations\n",
    "    :param machineannotations: the output annotations of the system in question\n",
    "    :type goldannotations: the type of the object created in extract_annotations\n",
    "    :type machineannotations: the type of the object created in extract_annotations\n",
    "    \n",
    "    :returns: a countainer providing the counts of true positives, false positives and false negatives for each class\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "288357f19699988c7abc534b62e3b64e",
     "grade": false,
     "grade_id": "cell-f5b5e06594d9b561",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_precision_recall_fscore(evaluation_counts):\n",
    "    '''\n",
    "    Calculate precision recall and fscore for each class and return them in a dictionary\n",
    "    \n",
    "    :param evaluation_counts: the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns the precision, recall and f-score of each class in a container\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9945c6016fa45c34c98f68897e213039",
     "grade": false,
     "grade_id": "cell-809e3e6e40314b9c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def carry_out_evaluation(goldannotations, systemfile, systemcolumn, sysdelimiter='\\t'):\n",
    "    '''\n",
    "    Carries out the evaluation process (from input file to calculating relevant scores)\n",
    "    \n",
    "    :param goldannotations: the gold annotations\n",
    "    :param systemfile: path to file with system output\n",
    "    :param systemcolumn: indication of column with relevant information\n",
    "    :param sysdelimiter: specification of formatting of file (default delimiter set to '\\t')\n",
    "    \n",
    "    returns evaluation information for this specific system\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd5dd7b9810afbe6b9d45f71498e67ca",
     "grade": false,
     "grade_id": "cell-4e4a9e60b9f57128",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def provide_output_tables(evaluations):\n",
    "    '''\n",
    "    Create tables based on the evaluation of various systems\n",
    "    \n",
    "    :param evaluations: the outcome of evaluating one or more systems\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e35dd9bde19a74d339bb9fe897bac553",
     "grade": false,
     "grade_id": "cell-754821695adcee64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def perform_evaluations(goldannotations, systems):\n",
    "    '''\n",
    "    Carry out standard evaluation for one or more system outputs\n",
    "    \n",
    "    :param goldfile: path to file with goldstandard\n",
    "    :param systemfiles: required information to find and process system output\n",
    "    :type goldfile: string\n",
    "    :type systemfiles: list (providing file name, information on tab with system output and system name for each element)\n",
    "    \n",
    "    :returns the evaluations for all systems\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34a5c1442c676a0e5ef9151be90473aa",
     "grade": false,
     "grade_id": "cell-df70262cf1bb1013",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def identify_evaluation_value(system, class_label, value_name, evaluations):\n",
    "    '''\n",
    "    Return the outcome of a specific value of the evaluation\n",
    "    \n",
    "    :param system: the name of the system\n",
    "    :param class_label: the name of the class for which the value should be returned\n",
    "    :param value_name: the name of the score that is returned\n",
    "    :param evaluations: the overview of evaluations\n",
    "    \n",
    "    :returns the requested value\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e54053c63b0eead6e20ad0d95a4e9a2",
     "grade": false,
     "grade_id": "cell-66b18bd41c5bb8f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Checking the overall set-up\n",
    "We will first test the evaluation scripts on a very small data set. You can carry out a small test yourself with the data provided in the data/ folder. A similar hidden test will be carried out by us for which the details (data) remains hidden for you. \n",
    "You are going to write a generic function that creates the gold annotations and calls the script to create all subsequent evaluations. \n",
    "It then becomes straight-forward to create the final setup of the system, that does not make use of a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4952cf40bb782f447b9320c6eb092043",
     "grade": false,
     "grade_id": "cell-359e77592b52408a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def run_evaluations(goldfile, gold_column, systemfiles):\n",
    "    '''\n",
    "    Create an evaluation where the gold is provided by data/minigold.csv and the system output by data/miniout1.csv\n",
    "    Use the label `system1' for the system\n",
    "    Label the evaluation values as 'precision', 'recall', 'f-score' (written full-out, all lowercase)\n",
    "    \n",
    "    :param goldfile: path to file with gold standard annotations\n",
    "    :param gold_column: indicator to identify the gold column\n",
    "    :param systemfiles: list that contains all information needed to process the systems\n",
    "    \n",
    "    :returns annotations\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "916a9d0b3758b3bee6516b0438124b42",
     "grade": false,
     "grade_id": "cell-cff55e09f184393d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_system_information(system_information):\n",
    "    '''\n",
    "    Takes system information in the form that it is passed on through sys.argv or via a settingsfile\n",
    "    and returns a list of elements specifying all the needed information on each system output file to carry out the evaluation.\n",
    "    \n",
    "    :param system_information is the input as from a commandline or an input file\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31a3d5aebb3fbaa0e0df4243f4c62b10",
     "grade": true,
     "grade_id": "cell-b6bb2fdc6fbea6b3",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "def main():\n",
    "    # these can come from the commandline using sys.argv for instance\n",
    "    my_args = ['data/minigold.csv','gold','data/miniout1.csv','NER','system1']\n",
    "    system_info = create_system_information(my_args[2:])\n",
    "    evaluations = run_evaluations(my_args[0], my_args[1], system_info)\n",
    "    provide_output_tables(evaluations)\n",
    "    check_eval = identify_evaluation_value('system1', 'O', 'f-score', evaluations)\n",
    "    assert_equal(\"%.3f\" % check_eval,\"0.889\")\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
