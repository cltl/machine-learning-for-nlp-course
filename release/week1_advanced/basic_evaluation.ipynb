{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7211cd7f4b9b7d96ec4409ae6655dddc",
     "grade": false,
     "grade_id": "cell-b3dce866d70241bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Basic Evaluation\n",
    "Create a set-up that can determine precision recall and f-score of the system output of multiple systems compared to a gold standard.\n",
    "This component should work with the output of your preprocessing component.\n",
    "It should be possible to call the evaluation script by specifing a gold file and system output files as input, together with any information necessary to correctly process the files.\n",
    "The file will also provide tables in latex form as output. You can choose whether you want to write this to a default outputfile or whether you want to ask for an (obligatory) argument specifying the outputfile.\n",
    "\n",
    "For verification reasons, the program should also output a file called 'evaluation_outcome.txt' that provides the outcome of the evaluation on individual lines. e.g.:\n",
    "system1 I-ORG precision 0.623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fa702d44394d4683ee2c84a3a772568",
     "grade": true,
     "grade_id": "cell-b6cce6ab0e0b2c3e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "def check_info_in_file(filename, text):\n",
    "    '''\n",
    "    checks whether a specific string is present in a file\n",
    "    :param filename: the name of the verification file\n",
    "    :param text: the text that is expected\n",
    "    \n",
    "    :returns a boolean indicating whether the text is found in the file or not\n",
    "    '''   \n",
    "    with open(filename) as f:\n",
    "        if text in f.read():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "#set command to run your program the second test should require the same system info as the first (if applicable)\n",
    "command = ''\n",
    "systemname = 'data/miniout1.csv'\n",
    "systeminfo = ''\n",
    "filename = 'evaluation_outcome.txt'\n",
    "text = 'data/miniout1.csv O f-score 0.889'\n",
    "os.system(command + systemname + systeminfo)\n",
    "\n",
    "#hint: if the variable check_eval stores the f-score, you obtain 3 decimals using \"%.3f\" % check_eval\n",
    "\n",
    "assert_equal(check_info_in_file(filename, text),True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
